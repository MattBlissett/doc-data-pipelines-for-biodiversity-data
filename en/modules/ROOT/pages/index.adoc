= Data Pipelines

== Process biodiversity data

Data Pipelines provides components to integrate, structure, interpret and transform biodiversity data. Built for extensibility, portability and high performance, data pipelines powers services such as https://www.gbif.org/[GBIF.org].

image::pipeline.png[align="center"]

=== Features

Some of the high-level capabilities and objectives of Data Pipelines include:

* Support a variety of input formats (https://www.tdwg.org/standards/dwc/[Darwin Core], https://www.tdwg.org/standards/abcd/[ABCD], CSV files, Excel files, Shapefiles etc) with easy opportunity to include new connectors
* Support batch (e.g. a project CSV) and streaming input (e.g. append-only tracking data)
* Align data to a standardized vocabularies, supporting multilingual data labelling
* Apply quality controls to flag errors, detect outliers and apply statements about the suitability of the data for a variety of uses (also known as fitness for use indicators)
* Enrich data by:
** cross referencing with geospatial gazetteers for political boundaries (e.g. https://gadm.org/[GADM.org], http://vliz.be/vmdcdata/marbound/[EEZ], protected areas) and biogeographic regions, land use categorization and environmental surfaces
** organize to multiple taxonomic classifications including the https://doi.org/10.15468/39omei[GBIF Backbone taxonomy], http://www.catalogueoflife.org/[Catalogue of Life] and national legislative taxonomies such as https://www.itis.gov/[ITIS]
* Allow consumers to easily understand the data preparation and enrichment process that has been applied (i.e. preserve and document data provenance).
* Provide clear documentation for all data transformations
* Support multiple runtime environments such as https://spark.apache.org/[Apache Spark], https://cloud.google.com/dataflow/[Google Dataflow], https://aws.amazon.com/emr/[Amazon EMR] or local machine
* Ensure pipelines can be deployed in a high throughput environment. https://www.gbif.org/[GBIF.org] target the processing and indexing into Elasticsearch of 1 billion records in under 12 hours
